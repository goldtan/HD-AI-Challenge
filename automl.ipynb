{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85ZWcmyreQNV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/Python/dacon/raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyiXZHJCeRtN"
      },
      "outputs": [],
      "source": [
        "!pip install autogluon"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load Data"
      ],
      "metadata": {
        "id": "A7bSlIYvRScZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install blpapi --index-url=https://bcms.bloomberg.com/pip/simple/\n",
        "# !pip install xbbg\n",
        "\n",
        "# from xbbg import blp\n",
        "\n",
        "# blp.bdp(tickers='BDIY IND', flds=['last_price'], start_date='2014-10-10', end_date='2023-10-18')"
      ],
      "metadata": {
        "id": "Hl42PnawbZPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz1SE1f1eT9L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "# 로그 스케일링 함수\n",
        "def log_scale(data, base=np.e):\n",
        "    return np.log(data + 1e-9) / np.log(base)\n",
        "\n",
        "def inverse_log_scale(log_data, base=np.e):\n",
        "    \"\"\"\n",
        "    Convert log scaled data back to its original scale.\n",
        "\n",
        "    Parameters:\n",
        "    - log_data: The log scaled data.\n",
        "    - base: The base of the logarithm. Default is the natural logarithm.\n",
        "\n",
        "    Returns:\n",
        "    - Data in its original scale.\n",
        "    \"\"\"\n",
        "\n",
        "    return np.power(base, log_data) - 1e-9\n",
        "\n",
        "# def cyclic_transform(data, col, max_val):\n",
        "#     data['sin_' + col] = np.sin(2 * np.pi * data[col] / max_val)\n",
        "#     data['cos_' + col] = np.sin(2 * np.pi * data[col] / max_val)\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Generate DateTime feature"
      ],
      "metadata": {
        "id": "D1QUT0yHRYj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df['time'] = pd.to_datetime(train_df['ATA'])\n",
        "# train_df['year'] = train_df['time'].dt.year\n",
        "# train_df['month'] = train_df['time'].dt.month\n",
        "# train_df['day'] = train_df['time'].dt.day\n",
        "# train_df['day_of_week'] = train_df['time'].dt.dayofweek\n",
        "\n",
        "# test_df['time'] = pd.to_datetime(test_df['ATA'])\n",
        "# test_df['year'] = test_df['time'].dt.year\n",
        "# test_df['month'] = test_df['time'].dt.month\n",
        "# test_df['day'] = test_df['time'].dt.day\n",
        "# test_df['day_of_week'] = test_df['time'].dt.dayofweek"
      ],
      "metadata": {
        "id": "rm4mcchDP3-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### + Imputing"
      ],
      "metadata": {
        "id": "fHFe8qGLRh6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# missing_rows_train = train_df[train_df.isnull().any(axis=1)]\n",
        "# missing_rows_test = test_df[test_df.isnull().any(axis=1)]\n",
        "\n",
        "# group = train_df.groupby(by = ['ARI_CO', 'month']).mean().loc[:,['U_WIND','V_WIND','AIR_TEMPERATURE','BN']]\n",
        "\n",
        "# #missing_rows['U_WIND'] = missing_rows.apply(lambda x: group.loc[x['ARI_CO'],x['month']]['U_WIND'], axis=1)\n",
        "# #missing_rows['V_WIND'] = missing_rows.apply(lambda x: group.loc[x['ARI_CO'],x['month']]['V_WIND'], axis=1)\n",
        "# missing_rows_train['AIR_TEMPERATURE'] = missing_rows_train.apply(lambda x: group.loc[x['ARI_CO'],x['month']]['AIR_TEMPERATURE'], axis=1)\n",
        "# missing_rows_test['AIR_TEMPERATURE'] = missing_rows_test.apply(lambda x: group.loc[x['ARI_CO'],x['month']]['AIR_TEMPERATURE'], axis=1)\n",
        "# #missing_rows['BN'] = missing_rows.apply(lambda x: group.loc[x['ARI_CO'],x['month']]['BN'], axis=1)"
      ],
      "metadata": {
        "id": "UCOh4mQ7PuRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Datetime to Cyclic"
      ],
      "metadata": {
        "id": "QLoyv1rURmwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cyclic_transform(data=train_df, col='ATA_LT', max_val=24)\n",
        "# cyclic_transform(data=train_df, col='month', max_val=12)\n",
        "# cyclic_transform(data=train_df, col='day', max_val=31)\n",
        "\n",
        "# cyclic_transform(data=test_df, col='ATA_LT', max_val=24)\n",
        "# cyclic_transform(data=test_df, col='month', max_val=12)\n",
        "# cyclic_transform(data=test_df, col='day', max_val=31)\n",
        "\n",
        "# train_df = train_df.drop(['ATA_LT', 'time', 'month', 'day'], axis=1)\n",
        "# test_df = test_df.drop(['ATA_LT', 'time', 'month', 'day'], axis=1)"
      ],
      "metadata": {
        "id": "NqYf6ywTJuFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Merge oil & ship index"
      ],
      "metadata": {
        "id": "b2cRSrdNRsi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['ATA_date'] = pd.to_datetime(train_df['ATA']).map(lambda x: x.date())\n",
        "test_df['ATA_date'] = pd.to_datetime(test_df['ATA']).map(lambda x: x.date())\n",
        "\n",
        "oil = pd.read_csv('oil/oil_data.csv', index_col = 0)\n",
        "oil['날짜'] = pd.to_datetime(oil['날짜']).map(lambda x: x.date())\n",
        "\n",
        "ship_idx = pd.read_csv('oil/ship_idx_data.csv', index_col = 0)\n",
        "ship_idx['날짜'] = pd.to_datetime(ship_idx['날짜']).map(lambda x: x.date())\n",
        "ship_idx = ship_idx.drop_duplicates(subset='날짜', keep = 'first')\n",
        "\n",
        "train_df = pd.merge(train_df, oil, left_on = \"ATA_date\", right_on =\"날짜\", how= 'left')\n",
        "test_df = pd.merge(test_df, oil, left_on = \"ATA_date\", right_on =\"날짜\", how= 'left')\n",
        "\n",
        "# 해상운임지수\n",
        "train_df = train_df.drop(['날짜'], axis=1)\n",
        "test_df = test_df.drop(['날짜'], axis=1)\n",
        "\n",
        "train_df = pd.merge(train_df, ship_idx, left_on = \"ATA_date\", right_on =\"날짜\", how= 'left')\n",
        "test_df = pd.merge(test_df, ship_idx, left_on = \"ATA_date\", right_on =\"날짜\", how= 'left')\n",
        "\n",
        "train_df = train_df.drop(['ATA', '날짜'], axis=1)\n",
        "test_df = test_df.drop(['ATA', '날짜'], axis=1)"
      ],
      "metadata": {
        "id": "XAV_50vxoE9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for df in [train_df, test_df]:\n",
        "  df[\"Volume\"] = df[\"LENGTH\"] * df[\"BREADTH\"] * df[\"DEPTH\"]\n",
        "  #df[\"Wind_Speed\"] = np.sqrt(df[\"U_WIND\"]**2 + df[\"V_WIND\"]**2)\n",
        "  #df[\"Wind_Direction\"] = np.arctan2(df[\"V_WIND\"], df[\"U_WIND\"]) * (180 / np.pi)  # Radian to Degree\n",
        "  df[\"Load_Capacity\"] = df[\"DEADWEIGHT\"] / df[\"GT\"]"
      ],
      "metadata": {
        "id": "UcCfRqOgaSZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IS_CI 컬럼 추가\n",
        "train_df['IS_CI'] = (train_df['CI_HOUR'] != 0).astype(int)"
      ],
      "metadata": {
        "id": "cfvkMm4inftS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 코드 (실제 데이터에 적용하기 전에 확인)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train_df['CI_HOUR'] = train_df['CI_HOUR'].map(log_scale)\n",
        "\n",
        "# 표준화\n",
        "scaler = StandardScaler()\n",
        "train_df['CI_HOUR'] = scaler.fit_transform(train_df['CI_HOUR'].values.reshape(-1, 1))\n",
        "\n",
        "for i in [\"ARI_CO\", \"ARI_PO\", \"FLAG\", \"SHIPMANAGER\"]:\n",
        "    # 각 범주에 대한 CI_HOUR의 평균과 분산 계산\n",
        "    grouped = train_df[train_df['CI_HOUR'] != 0].copy().groupby(i)['CI_HOUR'].agg(['mean', 'var'])\n",
        "    grouped.columns = [i+'_mean', i+'_var']\n",
        "\n",
        "    # 딕셔너리로 저장\n",
        "    mean_dict = grouped[i+'_mean'].to_dict()\n",
        "    var_dict = grouped[i+'_var'].to_dict()\n",
        "\n",
        "    # 딕셔너리를 사용하여 train_df에 매핑\n",
        "    train_df[i+'_mean'] = train_df[i].map(mean_dict)\n",
        "    train_df[i+'_var'] = train_df[i].map(var_dict)\n",
        "\n",
        "    # 딕셔너리를 사용하여 test_df에 매핑\n",
        "    test_df[i+'_mean'] = test_df[i].map(mean_dict)\n",
        "    test_df[i+'_var'] = test_df[i].map(var_dict)\n",
        "\n",
        "# 결과를 확인하기 위한 코드 (실제 실행에는 필요하지 않음)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "gMFk6weGhjW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o_vxs4ZgBji"
      },
      "outputs": [],
      "source": [
        "# # 'U_WIND', 'V_WIND', 'BN' 열에서 하나라도 결측치가 있는 행의 인덱스를 찾기\n",
        "# missing_train = train_df[train_df[['U_WIND', 'V_WIND', 'BN']].isnull().any(axis=1)].index\n",
        "# missing_test = test_df[test_df[['U_WIND', 'V_WIND', 'BN']].isnull().any(axis=1)].index\n",
        "\n",
        "# train_df_1 = train_df.iloc[~train_df.index.isin(missing_train)]\n",
        "# train_df_2 = train_df.iloc[missing_train]\n",
        "\n",
        "# test_df_1 = test_df.iloc[~test_df.index.isin(missing_test)]\n",
        "# test_df_2 = test_df.iloc[missing_test]\n",
        "\n",
        "# print(\"True or False if train_df_1 plus 2 equals train_df: \" + str(len(train_df) == len(train_df_1) +len(train_df_2)))\n",
        "# print(\"True or False if test_df_1 plus 2 equals test_df: \" + str(len(test_df) == len(test_df_1) +len(test_df_2)))\n",
        "\n",
        "# # autogluon 학습을 위한 데이터 형태로 변환\n",
        "# train_1 = TabularDataset(train_df_1.drop(['SAMPLE_ID','CI_HOUR', 'ID'], axis=1))\n",
        "# test_1 = TabularDataset(test_df_1.drop(['SAMPLE_ID', 'ID'], axis=1))\n",
        "\n",
        "# # autogluon 학습을 위한 데이터 형태로 변환\n",
        "# train_2 = TabularDataset(train_df_2.drop(['SAMPLE_ID','CI_HOUR','U_WIND', 'V_WIND', 'BN', 'ID'], axis=1))\n",
        "# test_2 = TabularDataset(test_df_2.drop(['SAMPLE_ID','U_WIND', 'V_WIND', 'BN','ID'], axis=1))\n",
        "\n",
        "# print(\"True or False if train_df_1 plus 2 equals train_df: \" + str(len(train_df) == len(train_1) +len(train_2)))\n",
        "# print(\"True or False if test_df_1 plus 2 equals test_df: \" + str(len(test_df) == len(test_1) +len(test_2)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = TabularDataset(train_df.drop(['SAMPLE_ID','ARI_CO','ARI_PO','CI_HOUR', 'U_WIND', 'V_WIND', 'AIR_TEMPERATURE','BN', 'ID', 'SHIPMANAGER', 'FLAG'], axis=1))\n",
        "train.columns"
      ],
      "metadata": {
        "id": "X-X8zQVBlLSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['IS_CI']==0]"
      ],
      "metadata": {
        "id": "E6Xgg6YVvPgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 이렇게 한 줄만 작성하면 내부에서 알아서 학습해줍니다.\n",
        "# predictor_clf_1 = TabularPredictor(label='IS_CI', eval_metric='f1').fit(train_1)\n",
        "# predictor_clf_2 = TabularPredictor(label='IS_CI', eval_metric='f1').fit(train_2)\n",
        "#predictor_clf = TabularPredictor(label='IS_CI', eval_metric='f1').fit(train, hyperparameters ={'XGB': {}})\n",
        "predictor_clf = TabularPredictor.load(\"AutogluonModels/ag-20231028_143508/\")\n",
        "#predictor_clf = TabularPredictor.load(\"AutogluonModels/ag-20231023_121256/\")"
      ],
      "metadata": {
        "id": "yCSEe5C6tr7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 보류\n",
        "# # IS_CI 컬럼 추가\n",
        "# train_df['OVER_100'] = (train_df['CI_HOUR'] > 100).astype(int)\n",
        "\n",
        "# train = TabularDataset(train_df[train_df['IS_CI'] == 1].drop(['SAMPLE_ID','CI_HOUR', 'U_WIND', 'V_WIND', 'AIR_TEMPERATURE','BN', 'ID', 'SHIPMANAGER', 'FLAG', 'IS_CI'], axis=1))\n",
        "# train.columns"
      ],
      "metadata": {
        "id": "JUxENn2xx1dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_reg = train_df[train_df['IS_CI'] == 1]"
      ],
      "metadata": {
        "id": "Q0GUnOx22TkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictor_clf_2 = TabularPredictor(label='OVER_100', eval_metric='recall').fit(train)\n",
        "# #predictor_clf_2 = TabularPredictor.load(\"AutogluonModels/ag-20231024_040233/\") #f1"
      ],
      "metadata": {
        "id": "NqgKO2uhxy7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GOAT\n",
        "# predictor_clf_1 = TabularPredictor.load(\"AutogluonModels/ag-20231022_082356/\")\n",
        "# predictor_clf_2 = TabularPredictor.load(\"AutogluonModels/ag-20231022_085339/\")\n",
        "\n",
        "# predictor = TabularPredictor.load(\"AutogluonModels/ag-20231022_113921/\")\n",
        "# predictor = TabularPredictor.load(\"AutogluonModels/ag-20231022_133024/\")"
      ],
      "metadata": {
        "id": "Tg_gpWgGLhsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # autogluon 학습을 위한 데이터 형태로 변환\n",
        "# train_1 = TabularDataset(train_df_1.drop(['SAMPLE_ID','CI_HOUR', 'ID'], axis=1))\n",
        "# test_1 = TabularDataset(test_df_1.drop(['SAMPLE_ID', 'ID'], axis=1))\n",
        "\n",
        "# # autogluon 학습을 위한 데이터 형태로 변환\n",
        "# train_2 = TabularDataset(train_df_2.drop(['SAMPLE_ID','CI_HOUR','U_WIND', 'V_WIND', 'BN', 'ID'], axis=1))\n",
        "# test_2 = TabularDataset(test_df_2.drop(['SAMPLE_ID','U_WIND', 'V_WIND', 'BN','ID'], axis=1))"
      ],
      "metadata": {
        "id": "vf1NnOgY7HB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # autogluon regression 학습을 위한 데이터 형태로 변환\n",
        "# train_1 = TabularDataset(train_df[train_df['IS_CI'] == 1].drop(['SAMPLE_ID','IS_CI', 'ID'], axis=1))\n",
        "# # autogluon regression 학습을 위한 데이터 형태로 변환\n",
        "# train_2 = TabularDataset(train_df[train_df['IS_CI'] == 1].drop(['SAMPLE_ID','IS_CI','U_WIND', 'V_WIND', 'BN', 'ID'], axis=1))"
      ],
      "metadata": {
        "id": "5VEc0P5D6W7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = TabularDataset(train_df_reg.drop(['SAMPLE_ID','ARI_CO','ARI_PO','IS_CI', 'U_WIND', 'V_WIND', 'AIR_TEMPERATURE','BN', 'ID', 'SHIPMANAGER', 'FLAG'], axis=1))\n",
        "train.columns"
      ],
      "metadata": {
        "id": "bnycejGW1qMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "Zs2Wnzx3BhqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 이렇게 한 줄만 작성하면 내부에서 알아서 학습해줍니다.\n",
        "# predictor_clf_1 = TabularPredictor(label='IS_CI', eval_metric='f1').fit(train_1)\n",
        "# predictor_clf_2 = TabularPredictor(label='IS_CI', eval_metric='f1').fit(train_2)"
      ],
      "metadata": {
        "id": "wVcLSmgp6_ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparam = {\n",
        "    'NN_TORCH': {},\n",
        "    'GBM': [\n",
        "        {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n",
        "        {},\n",
        "        'GBMLarge'\n",
        "    ],\n",
        "    #'CAT': {},\n",
        "    'XGB': {},\n",
        "    'FASTAI': {},\n",
        "    'RF': [\n",
        "        {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
        "        {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n",
        "        {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}}\n",
        "    ],\n",
        "    'XT': [\n",
        "        {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
        "        {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n",
        "        {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}}\n",
        "    ],\n",
        "    'KNN': [\n",
        "        {'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}},\n",
        "        {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}\n",
        "    ],\n",
        "    #'FT_TRANSFORMER': {}\n",
        "}"
      ],
      "metadata": {
        "id": "Zd-ePE1EPqy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이렇게 한 줄만 작성하면 내부에서 알아서 학습해줍니다.\n",
        "predictor_reg = TabularPredictor(label='CI_HOUR', eval_metric='mae').fit(train, presets = \"high_quality\", hyperparameters = hyperparam)\n",
        "# predictor_reg = TabularPredictor.load(\"AutogluonModels/ag-20231028_144913/\") 현재 최종 모델"
      ],
      "metadata": {
        "id": "xujJmryTB2Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgETMYtHgO5q"
      },
      "outputs": [],
      "source": [
        "# ld_board = predictor_reg.leaderboard(train, silent=True)\n",
        "# ld_board.to_excel(\"ld_board.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = TabularDataset(test_df.drop(['SAMPLE_ID','ARI_CO','ARI_PO', 'U_WIND', 'V_WIND', 'BN', 'ID'], axis=1))\n",
        "pred_class = predictor_clf.predict(test)\n",
        "pred_hour = predictor_reg.predict(test)"
      ],
      "metadata": {
        "id": "V8QCVUx1WnqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(train['CLASS']) == len(predictor_clf.predict(train)))\n",
        "# print(len(train['HOUR']) == len(predictor_reg.predict(train)))\n",
        "# print(len(train['CI_HOUR']) == len(train_df[train_df['IS_CI'] == 1]['CI_HOUR']))"
      ],
      "metadata": {
        "id": "ZRBnjOyqwN5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train['CLASS'] = predictor_clf.predict(train)\n",
        "# train['HOUR'] = predictor_reg.predict(train)\n",
        "# train['CI_HOUR'] = train_df[train_df['IS_CI'] == 1]['CI_HOUR']\n",
        "\n",
        "# train['HOUR'] = scaler.inverse_transform(train['HOUR'].values.reshape(-1,1))\n",
        "# train['HOUR'] = train['HOUR'].map(inverse_log_scale)\n",
        "# train['CI_HOUR'] = scaler.inverse_transform(train['CI_HOUR'].values.reshape(-1,1))\n",
        "# train['CI_HOUR'] = train['CI_HOUR'].map(inverse_log_scale)"
      ],
      "metadata": {
        "id": "lQjeJkINXxhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[['HOUR','CI_HOUR']].sort_values('CI_HOUR', ascending=False).tail(60)"
      ],
      "metadata": {
        "id": "kCAtPyXU7cog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[['HOUR','CI_HOUR']].describe()"
      ],
      "metadata": {
        "id": "3TEOa4quw6Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXC93f6Cg4q1"
      },
      "outputs": [],
      "source": [
        "# pred_class_1 = predictor_clf_1.predict(test_1)\n",
        "# pred_class_2 = predictor_clf_2.predict(test_2)\n",
        "# pred_hour_1 = predictor_reg_1.predict(test_1)\n",
        "# pred_hour_2 = predictor_reg_2.predict(test_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pred_class = pred_class_1.append(pred_class_2).sort_index()\n",
        "# pred_hour = pred_hour_1.append(pred_hour_2).sort_index()"
      ],
      "metadata": {
        "id": "hW-qp4By7953"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhJ9ZgyFhNki"
      },
      "outputs": [],
      "source": [
        "# 제출 파일 생성\n",
        "submit = pd.DataFrame()\n",
        "\n",
        "submit['SAMPLE_ID'] = test_df['SAMPLE_ID']\n",
        "submit['CI_HOUR'] = scaler.inverse_transform(pred_hour.values.reshape(-1,1))\n",
        "submit['CI_HOUR'] = submit['CI_HOUR'].map(inverse_log_scale)\n",
        "submit['IS_CI'] = pred_class\n",
        "\n",
        "# IS_CI 값이 0이면 CI_HOUR 값을 0으로 바꾸기\n",
        "submit.loc[submit['IS_CI'] == 0, 'CI_HOUR'] = 0\n",
        "submit.loc[submit['CI_HOUR'] < 0, 'CI_HOUR'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submit = submit.drop(['IS_CI'], axis=1)\n",
        "submit"
      ],
      "metadata": {
        "id": "JSt8x_4Cd9PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.to_csv(str(datetime.now())+ 'mae_submit.csv', index=False)"
      ],
      "metadata": {
        "id": "aZKteqnud1sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.describe()"
      ],
      "metadata": {
        "id": "yXgUzf6f4aRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.describe()"
      ],
      "metadata": {
        "id": "GdryOauHCXiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1KNWDf0ypzI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}